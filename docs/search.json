[
  {
    "objectID": "mora_hazard.html",
    "href": "mora_hazard.html",
    "title": "Optimal Compensation and Accounting",
    "section": "",
    "text": "Moral hazard arises when one party in a transaction can take actions that affect the value of the transaction for the other party. However, those actions are either unobservable or only imperfectly inferable by the affected party. This problem typically occurs in situations where two conditions are present: hidden action and risk aversion.\n\nHidden action: Refers to actions taken by one party that cannot be directly monitored or measured by the other party. As a result, the affected party cannot perfectly determine whether adverse outcomes are due to poor performance, lack of effort, or external factors.\nRisk aversion: At least one of the parties involved must prefer avoiding risk. They value stability and predictability, often making them cautious about transactions or agreements that expose them to unpredictable outcomes.\n\nThe moral hazard problem is fundamentally about the misalignment of incentives between parties. Specifically, the party whose actions are unobservable might not have adequate incentives to act in the best interest of the other party.\nSolving Moral hazard conflicts implies solving two challenges:\n\nProvision of adequate incentives: Designing mechanisms to motivate the party taking the hidden action to behave in a way that aligns with the other party’s goals.\nEfficient risk distribution: Finding a balance in how risk is shared between the parties, so that each party bears the appropriate level of risk without undermining incentives.\n\nThese two objectives often conflict. There is a substitution effect: when one party is overly shielded from risk, they may lack the incentive to take efficient actions that benefit the other party. As a result, designing an optimal compensation or incentive scheme requires balancing risk-sharing with providing the right incentives.\nThe focus of this class will be moral hazard conflicts arising within a firm, typically between a shareholder (the pricipal) and a manager (the agent). The principal aims to hire the agent for running a risky project, and offers the agent a contract which stablishes the agent’s compensation conditional on the outcome of the risky project. The agent first decides whether to accept the contract and then how much effort put into managing the project. The principal requires informations to infer the value of the project’s outcome and the agent’s effort to be able to compute the agent’s compensation and its own net profits.\nIn this setting, accounting is naturally embedded into moral hazard problems. Accounting information must have several key characteristics to be useful for decision-making. Accounting information must be relevant, comparable, verifiable, and timeliness. Scholars have studied how these characteristics can be used to design optimal contracts that mitigate moral hazard problems.\nThis reading note starts by developing a simple example of a moral hazard problem to illustrate the nature of the problem and the trade-offs involved in solving it. We then move on to a more general moral hazard model, allowing us to add further features to the model. We will also discuss how accounting systems must generate performance measures, and how these measures can be used to design optimal contracts. Lastly, we refer to the literature on relative performance evaluation and dynamic incentive contracts.\n\n\nLet’s assume that a company (principal) hires a manager (agent) responsible for running the business. The company’s outcomes depend on both random factors and the manager’s effort. For now, let’s assume the effort can be either high (H) or low (L), and the company’s outcomes can either be success (S) or failure (F). The company’s profit in the success scenario is $360, and in the failure scenario, it is $200. The probability of success is 75% with high effort and 25% with low effort.\nTherefore, the expected profit of the principal when effort is high is: 360 \\times 0.75 + 200 \\times 0.25 = 320. In contrast, when the effort is low the expected profit is: 360 \\times 0.25 + 200 \\times 0.75 = 240.\nThis can be summarized in the following table:\n\n\n\n\nHigh Effort (H)\nLow Effort (L)\n\n\n\n\nSuccess (S)\n75%\n25%\n\n\nFailure (F)\n25%\n75%\n\n\nExpected Profit\n$320\n$240\n\n\n\nWe assume that the principal seeks to maximize the expected value of their net profits (expected value minus the payment to the agent). For now, let’s consider the principal to be risk-neutral.\nThe agent (manager) aims to maximize an expected utility function of the form:\n\nU = U(w) - v(e)\n\nWhere U(w) is the utility of the payment received (remuneration), and v(e) is the cost of effort e.\nWe can distinguish four possible cases:\n\nObservable effort, risk-neutral agent\nNon-observable effort, risk-neutral agent\nObservable effort, risk-averse agent\nNon-observable effort, risk-averse agent\n\n\n\n\nLet’s assume an agent with a utility function U = w - v(e), and a reservation utility of \\underline{U} = 81. The costs of effort are v(L) = 0 and v(H) = 63. (The numbers are arbitrary and chosen to facilitate later comparisons.)\nIf the principal settles for low effort, they offer the agent a salary equal to the reservation utility w = 81. The expected profit for the principal is 240, and the net benefit is 240 - 81 = 159.\nIf the principal wants to ensure high effort, the agent’s remuneration must cover the disutility of effort, i.e., w = 81 + 63 = 144. The net profit for the principal in this case is 320 - 144 = 176.\nThus, for the principal, the best alternative is to offer a fixed payment of 144 and obtain an expected profit of 176.\n\n\n\nNow, consider the case where the effort is non-observable and the agent continue being risk-neutral. In this case, the conflict between incentives and risk distribution is easily resolved. The most effective way to provide incentives is to make the agent bear all the consequences of their decisions.\nThe contract cannot depend on effort because effort is neither observable nor verifiable. However, results such as profits are observable. Therefore, the principal can exploit the correlation between effort and profits to incentivize the agent. The principal will pay X in case of success (S), and Y in case of failure (F). This introduces uncertainty for the agent.\nThe principal can induce high effort by offering the agent a contract characterized by a compensation conditional on the outcome: W_E and W_F, with the following requirements:\n\nIndividual Rationality: The utility for the agent must be greater than their reservation utility.\nIncentive Compatibility: The utility of exerting high effort must be greater than the utility of low effort, i.e.,E[U(A)] &gt; E[U(B)].\n\nIn our example, the constraints are:\n\n0.75 w_S + 0.25 w_F - 63 \\geq 81\n \n0.75 w_S + 0.25 w_F - 63 \\geq 0.25 w_S + 0.75 w_F\n\nAs it is inefficient for the principal to pay more than needed to influence effort, the previous system of inequations are solved at the equality conditions. The optimal contract in this case involves:\n\nw_S = 184, \\quad w_F = 24.\n\n\n\n\nWhat happens when we have a risk-averse agent? For example, assume the utility function U = w^{1/2} - v(e), where v(H) = 3 and v(L) = 0, and the reservation utility is \\underline{U} = 9.\nTo induce low effort, it is sufficient to offer a fixed salary that yields a utility level equal to or greater than the reservation utility (w=\\underline{U}^2=81). The principal’s expected net profit is 240 - 81 = 159, the same as in the scenario with a risk-neutral agent.\nTo induce high effort, the agent needs to be compensated for the additional disutility of the high effort. As effort can be observed, this can be achieved with a fixed salary of 144=(\\underline{U}+3)^2, conditioned on high effort . Thus, the expected net profit for the principal is 320 - 144 = 176, higher than the profit for low effort (though it may not always be optimal to pay for high effort).\nIn this case, the agent’s risk aversion does not pose a problem since there is no uncertainty involved. The agent’s action and payment are not contingent on the state of nature.1\n\n\n\nThe lack of observability of effort forces to create the compensation contract contingent on outcomes (which are correlated with effort) or abandoning variable compensation altogether. In this scenario we can see the main conflict in moral hazard problems:\n\nEfficient Risk Distribution: the risk-neutral party (the principal) should bear all the risk, while the risk-averse party (the agent) remains on the certainty line.\nIncentive Problem: For proper incentives, the agent must perceive differences in remuneration based on their effort level.\n\nThe challenge is to find the optimal balance between incentive provision and risk distribution. In the case of a risk-neutral agent, this was not a significant issue, as simply transferring residual control to the agent solved the problem. However, in this case, any variable compensation scheme must compensate the risk-averse agent for the risk they bear.\nFor comparison, let’s keep the utility function and parameters from the previous section. If the principal desires low effort, a fixed salary of 81 is sufficient. However, to incentive high effort, the contract with w = 144 will no longer suffice because effort is not observable, and uniform wages provide an incentive to cheat by exerting low effort.\nThe contract that induces high effort must meet both the individual rationality and incentive compatibility constraints, expressed as:\n\n0.75U(w_S) + 0.25U(w_F) - 3 \\geq 9\n\n\n0.75U(w_S) + 0.25U(w_F) - 3 \\geq 0.25U(w_S) + 0.75U(w_S)\n\nThe second constraint can be written as 0.5[U(w_S) - U(w_F)] \\geq 3. This captures the difference in utilities associated with the remuneration in each state, ensuring that high effort is chosen over low effort. The solution for this system of equation is w_E = 182.25 and w_F = 56.25. The expected cost of the contract that incentives high effor for the principal is 0.75 \\times 182.25 + 0.25 \\times 56.25 = 159.75, leaving them with an expected net benefit of 320 - 159.75 = 169.25.\nIt is important to note that inducing high effort now has a higher cost for the principal compared to the observable effort case. The cost has increased from 144 to 159.75. The additional 6.75 is the cost of non-observability, which economically corresponds to compensating the agent for the risk assumed."
  },
  {
    "objectID": "mora_hazard.html#agents-problem",
    "href": "mora_hazard.html#agents-problem",
    "title": "Optimal Compensation and Accounting",
    "section": "2.1 Agent’s Problem",
    "text": "2.1 Agent’s Problem\nIn the final stage of the game, the agent selects an effort level e, affecting the probability distribution of n possible outcomes x_i. Let p_i(e) denote the probability of outcome x_i given effort e. The agent’s utility function is defined as U(w(x_i)) - v(e), where U(w(x_i)) is the utility derived from compensation w(x_i) for outcome x_i, and v(e) represents the disutility (cost) of effort.\nThe agent maximizes expected utility by choosing the effort level e that satisfies:\n\ne \\in \\arg\\max \\sum_{i=1}^n p_i(e)U(w(x_i)) - v(e)\n\nThis defines the incentive compatibility constraint (ICC), which ensures the agent has an incentive to choose the desired effort level.\nBefore exerting effort, the agent decides whether to participate by comparing the expected utility of the contract with their reservation utility \\underline{U} (outside option):\n\n\\sum_{i=1}^n p_i(e)U(w(x_i)) - v(e) \\geq \\underline{U}\n\nThis is the participation constraint (PC), which ensures that the agent prefers the contract over their outside option."
  },
  {
    "objectID": "mora_hazard.html#principals-problem",
    "href": "mora_hazard.html#principals-problem",
    "title": "Optimal Compensation and Accounting",
    "section": "2.2 Principal’s Problem",
    "text": "2.2 Principal’s Problem\nIn the first stage, the principal designs the contract, taking into account the agent’s optimal behavior (effort choice). The principal maximizes their expected benefit B(x_i - w(x_i)), which is the difference between the outcome x_i and the agent’s compensation w(x_i). The principal’s problem can be written as:\n\n\\begin{aligned}\n    \\max_{w(x_i), e} \\quad & \\sum_{i=1}^n p_i(e)[B(x_i - w(x_i))] &  \\\\\n    \\text{subject to:} \\quad & \\underline{U} \\leq \\sum_{i=1}^n p_i(e) U(w(x_i)) - v(e) & \\quad  \\text{(PC)} \\\\\n    & e \\in \\arg\\max \\left\\{ \\sum_{i=1}^n p_i(e) U(w(x_i)) - v(e) \\right\\} & \\quad  \\text{(ICC)}\n\\end{aligned}\n\\tag{1}"
  },
  {
    "objectID": "mora_hazard.html#first-order-simplification",
    "href": "mora_hazard.html#first-order-simplification",
    "title": "Optimal Compensation and Accounting",
    "section": "2.3 First-order Simplification",
    "text": "2.3 First-order Simplification\nSolving the ICC in Equation 1 can be complex. However, under certain conditions, the agent’s optimal effort can be determined by the first-order condition. Assuming the agent’s optimal effort is interior, the first-order condition is:\n\n\\sum_{i=1}^n p_i'(e)U(w(x_i)) - v'(e) = 0\n\nThis simplification is often used because it avoids solving the double maximization problem in Equation 1. However, the first-order condition is necessary but not sufficient. Since effort influences both disutility v(e) and the outcome probabilities p_i(e), the expected utility function may not be concave, meaning additional solutions (including suboptimal ones) could arise."
  },
  {
    "objectID": "mora_hazard.html#optimal-compensation-terms",
    "href": "mora_hazard.html#optimal-compensation-terms",
    "title": "Optimal Compensation and Accounting",
    "section": "2.4 Optimal Compensation Terms",
    "text": "2.4 Optimal Compensation Terms\nUsing the first-order condition approach, we can formulate the Lagrangian for the principal’s problem as:\n\nL(w(x_i),e) = \\sum_{i=1}^n p_i(e)[B(x_i - w(x_i))] + \\lambda \\left[ \\sum_{i=1}^n p_i(e) U(w(x_i)) - v(e) - \\underline{U} \\right] + \\mu \\left[ \\sum_{i=1}^n p_i'(e)U(w(x_i)) - v'(e) \\right]\n\nThe optimal contract can be characterized by taking the derivative of the Lagrangian with respect to w(x_i) for each outcome x_i, leading to the following condition:\n\n\\frac{B'(x_i - w(x_i))}{U'(w(x_i))} = \\lambda + \\mu \\frac{p_i'(e)}{p_i(e)}\n\\tag{2}\nIf \\mu = 0, the solution corresponds to the first-best solution, where the agent’s actions align perfectly with the principal’s objectives. However, as demonstrated by Holmström (1979), if the principal seeks to motivate effort beyond the minimum, \\mu &gt; 0. In this case, the optimal risk-sharing depends on the sign and magnitude of \\frac{p_i'(e)}{p_i(e)}.\nStatistically, \\frac{p_i'(e)}{p_i(e)} can be interpreted as how informative the outcome x_i is about the agent’s effort level e (Milgrom 1981). The principal rewards outcomes that are more likely when higher effort levels are exerted (p_i'(e) &gt; 0).\nFinally, the shape of the optimal contract depends on the principal’s and agent’s utility functions as well as the probability distribution of outcomes. In particular, if \\frac{p_i'(e)}{p_i(e)} is increasing in the outcome, satisfying the monotone likelihood ratio condition (MLRC), the contract is likely to be monotonic in the outcome, ensuring higher outcomes correspond to higher compensations.\n\n\n\n\n\n\nLinearity in the Outcome\n\n\n\nEven if the likelihood ratio is monotonic and linear in the outcome, this would not imply that the optimal compensation is also linear in the outcome. As Equation 2 shows, the shape of the contract also depends on the preference functions of the agent and the principal.\n\n\nFor example, assuming the the principal is risk neutral and the agent’s has an hyperbolic absolute risk aversion (HARA) utility function, then the optimal compensation is a concave function of the outcome, as shown in Equation 3.\n\n\\begin{align*}\nU(x) &=\\frac{1}{1-\\gamma}\\left(\\delta_0+\\delta_1 x\\right)^{(1-\\gamma)}\\\\\nw^*(x) &= -\\frac{ \\delta_0}{ \\delta_1 x} + \\delta_1^{(1-\\gamma)/\\gamma} \\left[ \\lambda + \\mu \\frac{p_i'(e)}{p_i(e)} \\right]^{1/\\gamma}\n\\end{align*}\n\\tag{3}\nThus, the optimal contract is linear in the outcome if \\gamma=1 (e.g., logarithmic utility function), and concave (convex) if \\gamma&gt;1 (\\gamma&lt;1).\n\n\n\n\n\n\nLimited Liability\n\n\n\nIn many instances, the optimal compensation needs to be bounded below given limited liability or wealth constraints of the agent. The problem is that Equation 2 implies penalization to the agent for bad outcomes. Therefore, the optimal compensation is often nondifferentiable and has a piecewise linear form. See Mirrlees (1974)."
  },
  {
    "objectID": "mora_hazard.html#sec-general_measures",
    "href": "mora_hazard.html#sec-general_measures",
    "title": "Optimal Compensation and Accounting",
    "section": "2.5 Accounting performance measures",
    "text": "2.5 Accounting performance measures\nNow let’s add an additional performance measure to the model to check in which conditions they reduced the welfare loss relative to the first-best solution. This will be the case if the new measure increases the agent’s incentive to exert effort or improve the risk-sharing between the principal and the agent. Be y the additional performance measure. In this case, Equation 2 becomes\n\n\\frac{B'(x_i - w(x_i,y_i))}{U'(w(x_i,y_i)) }= \\lambda + \\mu \\frac{p_i'(x,y;e)}{p_i(x,y;e)}\n\\tag{4}\nThus, the optimal contract depends on the performance metric if \\frac{p_i'(x,y;e)}{p_i(x,y;e)} also depends on y. However, if x is a sufficient statistic for x and y with respect to e, then the optimal contract is independent of y. This is called Holmstrom’s informativeness condition (Holmström 1979).\n\n\n\n\n\n\nSufficient Statistic\n\n\n\nIf x=e+a_1 and y=x+a_2, where a_1 and a_2 are independent random variables, then x is a sufficient statistic for x and y with respect to e. In other words, y does not add any information about e that is not already contained in x.\n\n\n\n2.5.1 Optimal Contract and Linear Aggregation\nAccounting systems aggregate performance measures to reduce the complexity of reporting every individual transaction or piece of non-financial information. The aggregation arises because it would be both costly and unrealistic to report all basic transactions. In practice, accountants aggregate these performance measures linearly, typically assigning equal weights to each component. For example, total cost is the sum of each cost item, and net benefits are calculated as revenues minus expenses. These aggregated measures are often used in performance evaluations and compensation schemes.\nHowever, the linear aggregation of performance measures with equal weights raises an important question: to what extent does contract theory support this approach? Banker and Datar (1989) provides theoretical support for the linear aggregation of performance measures but also highlights that the assumption of equal weights is rarely optimal. According to their analysis, the weights assigned to different components of a performance measure should reflect their signal-to-noise ratio (the sensitivity of the measure to effort relative to its variance). Only in cases where the signal-to-noise ratio is the same for all components can the equal weights assumption be considered optimal.\nTo illustrate this, consider the principal-agent framework where the principal is risk-neutral, and there are two performance metrics: x and y. We will continue considering x to be the outcome, which as an observable variable it can also play the role of a performance metric. The optimal contract is derived from Equation 4, which yields:\n\nw^*(x,y)= W \\left[ \\lambda + \\mu \\frac{p_i'(x,y;e)}{p_i(x,y;e)} \\right]\n\nwhere W is the inverse of the agent’s marginal utility function. We define a new compensation function s(\\pi) that depends on an aggregate performance measure \\pi=\\pi(x,y), where:\n\n\\pi(x,y)=\\lambda + \\mu \\frac{p_i'(x,y;e)}{p_i(x,y;e)}\n\nFor many probability distributions, such as those from the exponential family, the likelihood ratio is linear in both x and y. Therefore, the optimal contract supports the linear aggregation of performance metrics in those cases.\n\n\n2.5.2 Optimal Weights for Aggregation\nIn determining the optimal weights for these performance metrics, Banker and Datar (1989) shows that for the exponential family of distributions, when both metrics are independently distributed given the agent’s effort, the optimal weights are proportional to (1) the sensitivity of the metric to the agent’s effort and (2) the inverse of the variance of the performance metrics. Specifically, if we denote the weights as \\beta_x and \\beta_y, the optimal weights are given by:\n\n\\beta_x  = \\frac{\\frac{\\partial E(x;e)}{ \\partial e} var(y) }{ \\left[ var(x)+var(y) \\right]}\n\\tag{5}\nand\n\n\\beta_y  = \\frac{\\frac{\\partial E(y;e)}{ \\partial e} var(x) }{ \\left[ var(x)+var(y) \\right]}\n\nThe relative weights between the two performance metrics can be expressed as:\n\n\\frac{\\beta_x}{\\beta_y} = \\frac{\\frac{\\partial E(x;e)}{ \\partial e}}{var(x)} \\cdot \\frac{var(y)}{\\frac{\\partial E(y;e)}{ \\partial e}}\n\\tag{6}\nThis shows that the optimal weights for performance measures depend on both the sensitivity of each metric to the agent’s effort and the variance of the performance metrics. In cases where the signal-to-noise ratio differs across metrics, assigning equal weights leads to suboptimal outcomes. Therefore, a key takeaway is that optimal weighting adjusts based on the informational value of each performance measure in relation to the agent’s effort.\n\n\n\n\n\n\nThe relevance of variables not controlled by the agent\n\n\n\nIf the principal or other agent invest in technology or capital such as the agent’s effort is more productive or the performance metric is more sensitive to its effort, then it is optimal to introduce those variables into the contract even though are not controlled by the agent. In this case, the optimal contract adjusts the compensation to subtract the principal’s investment effect on the mean outcome or adjusts the weights of the performance metrics to reflect their new informativeness.\n\n\nHow the optimal contract should weights performance metrics that are correlated? In that case, Equation 6 is rewritten as\n\n\\frac{\\beta_x}{\\beta_y} = \\frac{ \\frac{\\partial E(x;e)}{\\partial e} - \\frac{cov(x,y)}{var(y)}\\frac{\\partial E(y;e)}{\\partial e} }{var(x)} \\frac{var(y)}{\\frac{\\partial E(y;e)}{\\partial e} - \\frac{cov(x,y)}{var(x)}\\frac{\\partial E(x;e)}{\\partial e}}\n\\tag{7}\nso, even if the agent does not control y (e.g., \\frac{\\partial E(y;e)}{\\partial e}=0) then both metrics should be included in the contract with weights that reflect the correlation between them.\n\n\\frac{\\beta_x}{\\beta_y} = -\\frac{var(y)}{covar(x,y)}\n\nThus, if the metrics are positively correlated, then their weights have opposite signs. The interpretation is that the positive correlation is driven by an exogenous common factor, and by adjusting y with a negative weight some of the noise driven by the common factor in x is removed."
  },
  {
    "objectID": "mora_hazard.html#agents-problem-1",
    "href": "mora_hazard.html#agents-problem-1",
    "title": "Optimal Compensation and Accounting",
    "section": "3.1 Agent’s Problem",
    "text": "3.1 Agent’s Problem\nUsing the property of normally distributed noise, \\mathbb{E}\\left[e^{k\\epsilon}\\right] = e^{k^{2}\\frac{\\sigma^{2}}{2}} for any k, we derive the agent’s expected utility:\n\n\\mathbb{E}\\left[u(w, e)\\right] = -e^{-\\eta\\left[t + se - \\frac{1}{2}ce^{2} - s^{2}\\eta\\frac{\\sigma^{2}}{2}\\right]}\n\nThe agent’s incentive compatibility constraint (ICC) can be reformulated as:\n\ne \\in \\arg\\max \\hat{w}(e) = t + se - \\frac{1}{2}ce^{2} - s^{2}\\eta\\frac{\\sigma^{2}}{2}\n\nSolving the first-order condition for effort, we obtain the optimal effort:\n\ne^* = \\frac{s}{c}\n\nThus, for any given incentive s, the principal can anticipate the agent’s effort level."
  },
  {
    "objectID": "mora_hazard.html#principals-problem-1",
    "href": "mora_hazard.html#principals-problem-1",
    "title": "Optimal Compensation and Accounting",
    "section": "3.2 Principal’s Problem",
    "text": "3.2 Principal’s Problem\nThe principal’s objective is to maximize their expected payoff, which is the performance outcome minus the agent’s compensation. The principal’s optimization problem is written as:\n\n\\begin{aligned}\n    \\underset{t,s,e}{\\max} & \\quad \\mathbb{E}[x(e^*) - t - s[x(e^*)]] &  \\\\\n    \\text{subject to:} & \\quad t + s[e^*] - \\frac{1}{2}c[e^*]^{2} - s^{2}\\eta\\frac{\\sigma^{2}}{2} = \\overline{w} & \\quad  \\text{(PC)} \\\\\n\\end{aligned}\n\\tag{8}"
  },
  {
    "objectID": "mora_hazard.html#optimal-compensation-terms-1",
    "href": "mora_hazard.html#optimal-compensation-terms-1",
    "title": "Optimal Compensation and Accounting",
    "section": "3.3 Optimal Compensation Terms",
    "text": "3.3 Optimal Compensation Terms\nSolving Equation 8 yields the optimal compensation terms for the fixed component t and the performance-related component s:\n\n\\begin{align*}\ns^{*} &= \\frac{1}{1 + c\\eta\\sigma^{2}} \\\\\nt^{*} &= \\overline{w} - \\frac{1 - c\\eta\\sigma^{2}}{2c\\left(1 + c\\eta\\sigma^{2}\\right)^{2}}\n\\end{align*}\n\nThe results indicate that the variable compensation (s^*) (performance-related incentive) is higher when:\n\nThe cost of effort c is low,\nThe agent’s degree of risk aversion \\eta is low,\nThe variance of the performance measure \\sigma^{2} is low.\n\nThe following graphs shows the comparative statics of the optimal contract terms and utilities for different values of \\sigma.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nc=0.8 # cost of effort\neta=1.5 # risk aversion\nw=1     # reservation utility\n# Plot s,t,and total, for different values of sigma \nsigma = np.linspace(0,2.5,10)\ns = 1/(1+c*eta*sigma**2)\nt = w - (1-c*eta*sigma**2)/(2*c*(1+c*eta*sigma**2)**2) \ntotal = t + s \neffort = s/c \nplt.plot(sigma, s, label='s')\nplt.plot(sigma, t, label='t')\nplt.plot(sigma, total, label='total')\nplt.plot(sigma, effort, label='effort', linestyle='dashed')\nplt.legend()\n# render sigma symbol in the x label\nplt.xlabel('Value of $\\sigma$')\nplt.ylabel('Value')\nplt.title('Optimal contract terms')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# plot the utility of the agent and principal for different values of sigma\nu_agent = -np.exp(-eta*(t+s*effort-0.5*c*effort**2-s**2*eta*sigma**2/2)) \nu_principal = effort-t-s*effort\nplt.plot(sigma, u_agent, label='Agent')\nplt.plot(sigma, u_principal, label='Principal')\nplt.legend()\nplt.xlabel('Value of $\\sigma$')\nplt.ylabel('Utility')\nplt.title('Agent and Principal utility')\nplt.show()"
  },
  {
    "objectID": "mora_hazard.html#accounting-perfomance-measures",
    "href": "mora_hazard.html#accounting-perfomance-measures",
    "title": "Optimal Compensation and Accounting",
    "section": "3.4 Accounting Perfomance Measures",
    "text": "3.4 Accounting Perfomance Measures\nAs in Section 2.5, assume that x is the outcome and y is an additional performance measure, such that:\n\nx = e + \\epsilon_x \\quad \\text{and} \\quad y = 2e + \\epsilon_y\n\nwhere \\epsilon_x and \\epsilon_y are jointly normally distributed with zero means. The joint distribution function f(x, y; e) is a bivariate normal distribution with the following properties:\n\nMean vector:\n\n\n\\mu_{x, y} = \\left(\\begin{array}{c} e \\\\ 2e \\end{array}\\right)\n\n\nCovariance matrix:\n\n\n\\Sigma_{x, y} = \\left(\\begin{array}{cc} \\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{array}\\right)\n\nwhere \\sigma_x^2 is the variance of \\epsilon_x, \\sigma_y^2 is the variance of \\epsilon_y, and \\rho is the correlation coefficient between \\epsilon_x and \\epsilon_y.\nNotice that the additional metric y is, on average, more sensitive to the agent’s effort than the outcome x, since:\n\n\\frac{\\partial E[y(e)]}{\\partial e} &gt; \\frac{\\partial E[x(e)]}{\\partial e}\n\nThis situation can arise in scenarios such as a merchandising company, where x represents net profits, but the agent has limited control over the cost of goods sold (e.g., limited negotiation power with suppliers), while the agent’s effort directly influences sales y.\n\n3.4.1 Aggregation of Performance Measures\nThe accounting system aggregates both metrics x and y into a single performance measure \\pi = \\pi(x, y), which is used to determine the optimal compensation terms w(\\pi). The relative weights for x and y, derived from Equation 7, are given by:\n\n\\frac{\\beta_x}{\\beta_y} = \\frac{1 - 2\\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_y^2}}{\\sigma_x^2} \\cdot \\frac{\\sigma_y^2}{2 - \\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2}}\n\nLet’s see the comparative statics of the optimal weights for different values of \\rho and \\sigma_y. First, let’s assume that both metrics have the same noise, \\sigma_x=\\sigma_y=1.25.\n\n\nCode\n# plot the beta_x/beta_y for different values of rho \nrho = np.linspace(-1,1,20)\nsigma_y = 1.25\nsigma_x = 1.25\n# subplot for rho\nbeta_rho = (1-2*(rho*sigma_x*sigma_y)/(sigma_y)**2)/(sigma_x**2)*(sigma_y**2)/(2-(rho*sigma_x*sigma_y)/(sigma_x)**2)\nplt.plot(rho, beta_rho)\nplt.xlabel('correlation coefficient')\n# add vertical line in x=0\nplt.axvline(x=0, color='r', linestyle='--')\n## add subtitle\nplt.title('Relative Optimal Weights')\nplt.show()\n\n\n\n\n\n\n\n\n\nso, if \\rho=0, the optimal weight for x is half of the weight for y, as this second signal is more sensitive to effort and has the same noise. If \\rho=-1, then both both signal have full positive weight in the contract. If \\rho=1, then the signals are substracted from each other in the contract to remove the common noise.\nNow, let’s see the comparative statics for different values of \\sigma_y, considering \\rho=0 and \\sigma_x=1.25.\n\n\nCode\n# plot the beta_x/beta_y for different values of sigma_y\nrho=0\nsigma_y = np.linspace(0.1,2.5,10)\nsigma_x = 1.25\n# plot for sigma_y\nbeta_sigma_y = (1-2*(rho*sigma_x*sigma_y)/(sigma_y)**2)/(sigma_x**2)*(sigma_y**2)/(2-(rho*sigma_x*sigma_y)/(sigma_x)**2)\nplt.axvline(x=1.25, color='r', linestyle='--')\nplt.plot(sigma_y, beta_sigma_y)\nplt.xlabel('Value of $\\sigma_y$')\nplt.title('Relative Optimal Weights')\nplt.show()\n\n\n\n\n\n\n\n\n\nHere the red line despicts the same situation than in the previous plot, meaning that when both signals have the same noise. The optimal weight for x is half of the weight for y. If \\sigma_y gets closer to zero, then the weight for x decreases as y gets a better relative signal-to-noise ratio."
  },
  {
    "objectID": "mora_hazard.html#optimal-contract-with-additional-performance-measures",
    "href": "mora_hazard.html#optimal-contract-with-additional-performance-measures",
    "title": "Optimal Compensation and Accounting",
    "section": "3.5 Optimal Contract with additional Performance Measures",
    "text": "3.5 Optimal Contract with additional Performance Measures\nTo evaluate how the optimal contract changes with the use of the aggregate performance metric we should compute the aggregated signal \\pi(x,y)=\\beta_x x+ \\beta_y y using Equation 5. For simplicity, assume \\rho=0.\n\n\\pi(x,y)=\\frac{\\sigma_y^2}{\\sigma_x^2+\\sigma_y^2} x+ 2 \\frac{\\sigma_x^2}{\\sigma_x^2+\\sigma_y^2} y\n\nNow, the compensation functions depens on the aggregated signal \\pi (rather than on the output x), meaning that now w(\\pi)=t+s\\pi. Notice that \\pi \\sim N(\\mu_{\\pi},\\sigma_{\\pi}^2), with\n\n\\begin{align*}\n\\mu_{\\pi}(e) &= \\frac{e}{\\sigma_x^2+\\sigma_y^2} \\left[  \\sigma_y^2+4\\sigma_x^2 \\right]\\\\\n\\sigma_{\\pi}^2(e) &= \\frac{\\sigma_x^2\\sigma_y^2}{(\\sigma_x^2+\\sigma_y^2)^2}\\left[  \\sigma_y^2+4\\sigma_x^2 \\right]\n\\end{align*}\n\nWe can reformulate agent’s problem as \ne \\in\\arg\\max\\hat{w}(e)=t+s\\mu_{\\pi}-\\frac{1}{2}ce^{2}-s^{2}\\eta\\frac{\\sigma_{\\pi}^2 }{2}\n\nobtaining optimal effort equals to \ne^*=\\frac{s}{c} \\frac{\\partial \\mu_{\\pi}(e)}{\\partial e} =\\frac{s}{c} \\left[\\frac{\\sigma_y^2+4\\sigma_x^2}{\\sigma_x^2+\\sigma_y^2} \\right]\n\\tag{9}\nAgain, for any given performance incentive s, the principal can infer the optimal effort of the agent. Relative to our initial result (e^*=s/c), Equation 9 shows that having the aggregated metric \\pi in the contract increases the optimal effort level for any given value of performance incentive s as much as the sensitivity of expected value of \\pi to the agent’s effort is higher than one.\nThe updated principal’s problem is \n\\begin{align*}\n\\underset{t, s}{\\max} \\quad & \\mathbb{E}[x(e^*) - t - s \\cdot x(e^*)] \\\\\n\\text{subject to:} \\quad & t + s \\cdot e^* - \\frac{1}{2} c (e^*)^2 - s^2 \\eta \\frac{\\sigma_{\\pi}^2(e^*)}{2} = \\overline{w}\n\\end{align*}\n Obtaining\n\ns^* = \\frac{\\frac{\\partial \\mu_{\\pi}(e)}{\\partial e}}{\\left(\\frac{\\partial \\mu_{\\pi}(e)}{\\partial e}\\right)^2+c n \\sigma_{\\pi}^2} =\\frac{\\frac{\\sigma_y^2 + 4\\sigma_x^2}{\\sigma_x^2 + \\sigma_y^2}}{\\left( \\frac{\\sigma_y^2 + 4\\sigma_x^2}{\\sigma_x^2 + \\sigma_y^2} \\right)^2 + c n \\sigma_{\\pi}^2}\n\nThe next figure compares the optimal effort and compensation terms for different values of \\sigma_x with our previous result for the case without the additional metric.\n\n\nCode\nc=0.8 # cost of effort\neta=1.5 # risk aversion\nw=1     # reservation utility\n# initial contract\nsigma_x = np.linspace(0,2.5,10)\ns_old = 1/(1+c*eta*sigma_x**2)\nt_old = w - (1-c*eta*sigma_x**2)/(2*c*(1+c*eta*sigma_x**2)**2) \neffort_old = s_old/c \n# new contract\nsigma_y = 1.25\nsigma_pi2= (sigma_x**2*sigma_y**2)/((sigma_x**2+sigma_y**2)**2)*(sigma_y**2+4*sigma_x**2)\npi_sensitivity = (sigma_y**2+4*sigma_x**2)/(sigma_x**2+sigma_y**2)\ns_new= pi_sensitivity/(pi_sensitivity**2+c*eta*sigma_pi2)\neffort_new= (s_new/c)*pi_sensitivity\n\nplt.plot(sigma_x, s_old, label='s', linestyle='solid',color='blue')\nplt.plot(sigma_x, effort_old, label='effort', linestyle='dashed' ,color='blue')\nplt.plot(sigma_x, s_new, label='s with additional metric', linestyle='solid',color='green')\nplt.plot(sigma_x, effort_new, label='effort with additional metric', linestyle='dashed', color='green')\nplt.axvline(x=1.25, color='r', linestyle='--')\nplt.legend()\n# render sigma symbol in the x label\nplt.xlabel('Value of $\\sigma_x$')\nplt.ylabel('Value')\nplt.title('Optimal contract terms')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see that the introduction of the additional metric y leads to two effects. First, the optimal effort increases regardless of how noisy is the outcome. Second, performance-related incentive s is lower than the original case for low values of outcome volatility, but is larger when this volatility is high. This is because for low outcome volatility, the new metric is more sensitive to the agent’s effort than the output x alone, being more efficient to motivate the agent. However, for high outcome volatility, the new metric is less sensitive to the agent’s effort, so the principal needs to increase the incentive to motivate the agent."
  },
  {
    "objectID": "mora_hazard.html#suggestions-for-further-reading",
    "href": "mora_hazard.html#suggestions-for-further-reading",
    "title": "Optimal Compensation and Accounting",
    "section": "3.6 Suggestions for Further Reading",
    "text": "3.6 Suggestions for Further Reading\nThe literature on moral hazard has expanded the basic model in several important directions. One strand of research introduces stock-price incentives into compensation contracts, as explored by Kim and Suh (1991), Paul (1992), Dutta and Reichelstein (2005), and Bernardo et al. (2015).\nAnother line of research extends the set of actions available to managers, as highlighted by Holmstrom and Milgrom (1991), Feltham and Xie (1994), Bushman and Indjejikian (1993), and Paul (1992).\nFinally, dynamic models of moral hazard have also been developed. Notable contributions in this area include Holmstrom and Milgrom (1987), Dutta and Reichelstein (1999), Corona (2009), Cvitanić et al. (2016), and Ai et al. (2023)."
  },
  {
    "objectID": "mora_hazard.html#principals-problem-2",
    "href": "mora_hazard.html#principals-problem-2",
    "title": "Agency Theory and Accounting",
    "section": "4.1 Principal’s Problem",
    "text": "4.1 Principal’s Problem\nWe start with the scenario where the agent acquires private information after signing the contract but before choosing his action. He is unable to leave after observing the signal and cannot communicate the signal to the principal. After receiving the private information, the agent can adjust his effort level on the signal, meaning e(m). The optimal compensation w(x,y), cannot depend on the signal m because the agent cannot communicate it to the principal.\nThe principal’s problem is\n\n\\begin{aligned}\n    \\max_{w(x,y),e(m)} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y)] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y, m} \\left[ U[w(x, y)| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m, \\quad & \\text{(PCs)}\\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y) | e] - V(e) \\right] \\text{ for each signal } m \\quad & \\text{(ICCs)}\n\\end{aligned}\n\nNotice that now the principal’s problem include a set of PCs and ICCs, each of them for a different signal m. To avoid that the agent to leave after observing bad realizations of the signal, the principal should offer a contract that is better than the outside options for every realization, not just in expectations. This implies that the principal obtain an excess level of utility, called information rent, derived from the fact that the agent earns the minimal acceptable level of utility just in the realization of the worst signal, and earns in excess to this minimal level in the other realizations. The principal can reduce the information rent by forcing the agent to truthfully report the private information. In those cases, the principal can use this information as an additional metric to adjust the contract to reduce the information rent. However, given the private nature of the information and the incentive scheme, the principal knows that the agent has incentive to misreport the signal. Let \\hat{m}(m) be the report that the agent sends after observing m. The principal can design a contract that depends on the reported signal, w(x,y,\\hat{m}). The principal’s problem is then\n\n\\begin{aligned}\n    \\max_{w(x,y),e(m), \\hat{m}(m)} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,\\hat{m})] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,\\hat{m})| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\hat{m}) | e] - V(e) \\right] \\text{ for each } m \\quad  \\\\\n    & \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\hat{m}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nThe first set of constraints consists of the minimal acceptable utility constraints, the second set comprises the incentive compatibility constraints on the agent’s effort, and the third set includes the incentive compatibility constraints on the agent’s reporting strategy."
  },
  {
    "objectID": "mora_hazard.html#relevation-principle",
    "href": "mora_hazard.html#relevation-principle",
    "title": "Agency Theory and Accounting",
    "section": "4.2 Relevation Principle",
    "text": "4.2 Relevation Principle\nAs the set of possible strategies is very large, the problem for the principal is difficult to solve. A shorcut developed in the literature was the Relevation Principle. The revelation principle states that any proposed mechanism that involves nontruthful reporting by the agent can be duplicated or beaten in terms of expected utilities by an equilibrium mechanism in which truthful reporting is induced. The cost for forcing the agent to tell the trut is that the principal must commit to not use the information as fully as he would if the truthful message did not have to be motivated. The revelation principle is a very helpful tool for researchers because it reduces the number of alternative reporting strategies needed to be considered. This allows researchers to focus on reporting strategies that encourage truthful reporting. To apply the relevation principal to our problem, we should substitute the report by the true signal, $=m, and set the contract such that it ensures that the best reporting strategy for the agent is to tell the truth.\n\n\\begin{aligned}\n    \\max_{w(x,y),e(m), \\hat{m}(m)} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,m)] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,m)| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,m) | e] - V(e) \\right] \\text{ for each } m \\quad  \\\\\n    & m(m) \\text{ is the } \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\hat{m}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nIn these models, the principal’s welfare loss is due to the information rent of the agent, which exists even in contexts with risk neutral agents. For this reason, mos of the model examine private information issues using risk neutral agents, so they can avoid the complexities that result when risk aversion and risk sharing issues are also present."
  },
  {
    "objectID": "mora_hazard.html#aplication-to-capital-budgeting",
    "href": "mora_hazard.html#aplication-to-capital-budgeting",
    "title": "Agency Theory and Accounting",
    "section": "4.3 Aplication to Capital Budgeting",
    "text": "4.3 Aplication to Capital Budgeting\nPending"
  },
  {
    "objectID": "mora_hazard.html#aplication-to-transfer-pricing",
    "href": "mora_hazard.html#aplication-to-transfer-pricing",
    "title": "Agency Theory and Accounting",
    "section": "4.4 Aplication to Transfer Pricing",
    "text": "4.4 Aplication to Transfer Pricing\nPending"
  },
  {
    "objectID": "mora_hazard.html#footnotes",
    "href": "mora_hazard.html#footnotes",
    "title": "Optimal Compensation and Accounting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdditionally, if the agent has bargaining power, they could negotiate up to 161 for high effort. Analyse the Edgeworth box representation.↩︎"
  },
  {
    "objectID": "private_inf.html",
    "href": "private_inf.html",
    "title": "Private Information and Incentives",
    "section": "",
    "text": "In this section we discuss models where the agent has private information that is not observable to the principal, which introduces challenges in aligning incentives. The agent’s private information can relate to various factors, such as their skill, expertise, or the profitability of investment opportunities. This informational asymmetry means that the agent holds an information rent which allows him to potentially extract more value than he could if the principal were fully informed. The key challenge for the principal is to design contracts that minimize these rents while still providing incentives for the agent to act in the firm’s best interest.\nLet x and y be the outcome and additional metric, both observable at the end of the game. The agent’s private information signal m has a priori probability density function g(m). As before, the agent’s effort is e. Once the signal is received, the density function of the outcome and the other metric is updated to be f(x,y|e,m). The models differs in three aspects: First, when does the agent receive the signal m, second, whether the agent can leave the contract after observing m, and third, whether the agent can communicate m to the principal in a truthtfull way.\n\n\nWe start with the scenario where the agent acquires private information after signing the contract but before choosing his action. He is unable to leave after observing the signal and cannot communicate the signal to the principal. After receiving the private information, the agent can adjust his effort level on the signal, meaning e(m). The optimal compensation w(x,y) cannot depend on the signal m because the agent cannot communicate it to the principal.\nThe principal’s problem is\n\n\\begin{aligned}\n    \\max_{w(x,y),e(m)} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y)] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y, m} \\left[ U[w(x, y)| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m, \\quad & \\text{(PCs)}\\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y) | e] - V(e) \\right] \\text{ for each } m \\quad & \\text{(ICCs)}\n\\end{aligned}\n\nNotice that now the principal’s problem include a set of PCs and ICCs, each of them for a different signal m. To avoid that the agent to leave after observing bad realizations of the signal, the principal should offer a contract that is better than the outside options for every realization, not just in expectations. This implies that the agent obtains an excess level of utility, called information rent, derived from the fact that the agent earns the minimal acceptable level of utility just in the realization of the worst signal, and earns in excess to this minimal level in the other realizations. The principal can reduce the information rent by forcing the agent to truthfully report the private information. In those cases, the principal can use this information as an additional metric to adjust the contract to reduce the information rent.\nHowever, given the private nature of the information and the incentive scheme, the principal knows that the agent has incentive to misreport the signal. Let \\color{blue}\\hat{m}(m) be the report that the agent sends after observing m. The principal can design a different contract that depends on the reported signal, w(x,y,\\textcolor{blue}{\\hat{m}}). The principal’s problem is then\n\n\\begin{aligned}\n    \\max_{w(x,y,\\textcolor{blue}{\\hat{m}}),e(m), \\textcolor{blue}{\\hat{m}(m)}} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,\\textcolor{blue}{\\hat{m}})] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}})| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}}) | e] - V(e) \\right] \\text{ for each } m, \\quad  \\\\\n    & \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nThe first set of constraints consists of the minimal acceptable utility constraints, the second set comprises the incentive compatibility constraints on the agent’s effort, and the third set includes the incentive compatibility constraints on the agent’s communication strategy.\n\n\n\nAs the set of possible strategies is very large, the problem for the principal is difficult to solve. A shorcut developed in the literature is the Relevation Principle. The revelation principle states that any proposed mechanism that involves nontruthful communication by the agent can be duplicated or beaten in terms of expected utilities by an equilibrium mechanism in which truthful reporting is induced. The cost for forcing the agent to tell the trut is that the principal must commit to not use the information as fully as he would if the truthful message did not have to be motivated. The revelation principle is a very helpful tool for researchers because it reduces the number of alternative reporting strategies needed to be considered. This allows researchers to focus on reporting strategies that encourage truthful reporting. To apply the relevation principal to our problem, we should substitute the report by the true signal, \\textcolor{blue}{\\hat{m}}=\\textcolor{green}{m}, and set the contract such that it ensures that the best reporting strategy for the agent is to tell the truth.\n\n\\begin{aligned}\n    \\max_{w(x,y,\\textcolor{green}{m}),e(\\textcolor{green}{m}), \\textcolor{green}{m}} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,\\textcolor{green}{m})] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,\\textcolor{green}{m})| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{green}{m}) | e] - V(e) \\right] \\text{ for each } m \\quad  \\\\\n    & m(m) \\text{ is the } \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{green}{m}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nIn these models, the principal’s welfare loss is due to the information rent of the agent, which exists even in contexts with risk neutral agents. For this reason, mos of the model examine private information issues using risk neutral agents, so they can avoid the complexities that result when risk aversion and risk sharing issues are also present."
  },
  {
    "objectID": "private_inf.html#principals-problem",
    "href": "private_inf.html#principals-problem",
    "title": "Private Information and Incentives",
    "section": "",
    "text": "We start with the scenario where the agent acquires private information after signing the contract but before choosing his action. He is unable to leave after observing the signal and cannot communicate the signal to the principal. After receiving the private information, the agent can adjust his effort level on the signal, meaning e(m). The optimal compensation w(x,y) cannot depend on the signal m because the agent cannot communicate it to the principal.\nThe principal’s problem is\n\n\\begin{aligned}\n    \\max_{w(x,y),e(m)} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y)] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y, m} \\left[ U[w(x, y)| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m, \\quad & \\text{(PCs)}\\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y) | e] - V(e) \\right] \\text{ for each } m \\quad & \\text{(ICCs)}\n\\end{aligned}\n\nNotice that now the principal’s problem include a set of PCs and ICCs, each of them for a different signal m. To avoid that the agent to leave after observing bad realizations of the signal, the principal should offer a contract that is better than the outside options for every realization, not just in expectations. This implies that the agent obtains an excess level of utility, called information rent, derived from the fact that the agent earns the minimal acceptable level of utility just in the realization of the worst signal, and earns in excess to this minimal level in the other realizations. The principal can reduce the information rent by forcing the agent to truthfully report the private information. In those cases, the principal can use this information as an additional metric to adjust the contract to reduce the information rent.\nHowever, given the private nature of the information and the incentive scheme, the principal knows that the agent has incentive to misreport the signal. Let \\color{blue}\\hat{m}(m) be the report that the agent sends after observing m. The principal can design a different contract that depends on the reported signal, w(x,y,\\textcolor{blue}{\\hat{m}}). The principal’s problem is then\n\n\\begin{aligned}\n    \\max_{w(x,y,\\textcolor{blue}{\\hat{m}}),e(m), \\textcolor{blue}{\\hat{m}(m)}} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,\\textcolor{blue}{\\hat{m}})] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}})| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}}) | e] - V(e) \\right] \\text{ for each } m, \\quad  \\\\\n    & \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{blue}{\\hat{m}}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nThe first set of constraints consists of the minimal acceptable utility constraints, the second set comprises the incentive compatibility constraints on the agent’s effort, and the third set includes the incentive compatibility constraints on the agent’s communication strategy."
  },
  {
    "objectID": "private_inf.html#relevation-principle",
    "href": "private_inf.html#relevation-principle",
    "title": "Private Information and Incentives",
    "section": "",
    "text": "As the set of possible strategies is very large, the problem for the principal is difficult to solve. A shorcut developed in the literature is the Relevation Principle. The revelation principle states that any proposed mechanism that involves nontruthful communication by the agent can be duplicated or beaten in terms of expected utilities by an equilibrium mechanism in which truthful reporting is induced. The cost for forcing the agent to tell the trut is that the principal must commit to not use the information as fully as he would if the truthful message did not have to be motivated. The revelation principle is a very helpful tool for researchers because it reduces the number of alternative reporting strategies needed to be considered. This allows researchers to focus on reporting strategies that encourage truthful reporting. To apply the relevation principal to our problem, we should substitute the report by the true signal, \\textcolor{blue}{\\hat{m}}=\\textcolor{green}{m}, and set the contract such that it ensures that the best reporting strategy for the agent is to tell the truth.\n\n\\begin{aligned}\n    \\max_{w(x,y,\\textcolor{green}{m}),e(\\textcolor{green}{m}), \\textcolor{green}{m}} \\quad & \\mathbb{E}_{x, y, m} \\left[ G[x - w(x,y,\\textcolor{green}{m})] | e(m) \\right] \\\\\n    \\text{subject to} \\quad & \\mathbb{E}_{x, y| m} \\left[ U[w(x, y,\\textcolor{green}{m})| e(m)] - V[e(m)] \\right] \\geq \\underline{U} \\text{ for all } m,  \\\\\n    & e(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{green}{m}) | e] - V(e) \\right] \\text{ for each } m \\quad  \\\\\n    & m(m) \\text{ is the } \\hat{m}(m) \\in \\arg\\max \\mathbb{E}_{x, y | m} \\left[ U[w(x, y,\\textcolor{green}{m}) | e] - V(e) \\right] \\text{ for all } m \\\\\n\\end{aligned}\n\nIn these models, the principal’s welfare loss is due to the information rent of the agent, which exists even in contexts with risk neutral agents. For this reason, mos of the model examine private information issues using risk neutral agents, so they can avoid the complexities that result when risk aversion and risk sharing issues are also present."
  },
  {
    "objectID": "private_inf.html#aplication-to-capital-budgeting",
    "href": "private_inf.html#aplication-to-capital-budgeting",
    "title": "Private Information and Incentives",
    "section": "",
    "text": "For an early review of this models, see Antle and Fellingham (1997). The model assumed that the agent learn privately the minimal investment amount that will yield a given cash flow x (the outcome of the risky project). To obtain this cash flow, it is needed to initially invest mx. The principal provides the capital z that cover the investment and the agent’s compensation w, but the agent can also divert a fraction of z for its own benefit.\nThe assume a discrete number of potential information signals that the agent can privately observe: m \\in \\{m_1, m_2, \\ldots, m_n\\}."
  },
  {
    "objectID": "private_inf.html#aplication-to-transfer-pricing",
    "href": "private_inf.html#aplication-to-transfer-pricing",
    "title": "Private Information and Incentives",
    "section": "2.3 Aplication to Transfer Pricing",
    "text": "2.3 Aplication to Transfer Pricing\nPending"
  },
  {
    "objectID": "private_inf.html#first-best-solution",
    "href": "private_inf.html#first-best-solution",
    "title": "Private Information and Incentives",
    "section": "2.2 First Best Solution",
    "text": "2.2 First Best Solution\nLet’s ignore for now the information assymetry in the true cost of the project m. The principal’s problem is to maximizes the expected value of net profits. Normalizing the unit value of output to be 1, the principal’s problem is\n\n\\begin{aligned}\n    \\max_{x_i, z_i} \\quad & \\sum_{i=1}^{N} (x_i - z_i) g_i \\\\\n    \\text{subject to} \\quad & z_i - m_i x_i \\geq \\underline{U} \\quad \\text{for all } i = 1, \\dots, N, \\\\\n    & 0 \\leq x_i \\leq x_{\\max}, \\\\\n    & 0 \\leq z_i.\n\\end{aligned}\n\\tag{1}\nThus the problem for the principal is to find the optimal combination of (cash flow, transference) (x_i,z_i) that maximizes the expected value of the principal’s profits subject to the agent’s minimal acceptable utility constraint in any scenario of the cost of the project.\nIn Equation 1, the first restriction is the minimal acceptable utility constraint for the agent regardless of the signal observed. The second and third restrictions set bounds on the cash flows and transference for the solution to exist (notice the linearity in the production technology and lack of agent’s wealth).\nThe first restrictions implies that z_i^*=\\underline{U}+m_i x_i. Inserting this into the objective function, we obtain\n\n\\begin{aligned}\n    \\max_{x_i} \\quad & \\sum_{i=1}^{N} (1-m_i)(x_i g_i)-\\underline{U}  \\\\\n\\end{aligned}\n\nwhich is maximized when x_i=x_{\\max} if m_i\\leq 1 and x_i=0 if m_i&gt; 1. The intuition is that the expected value of the principal’s profits is at its maximum when the project generates the highest possible cash flows if the signal indicates that the project is profitable. Consistently, the optimal transfer is\n\nz_i =\n\\begin{cases}\n    m_i x_{\\max} + \\underline{U} & \\text{for } m_i \\leq 1, \\\\\n    \\underline{U} & \\text{for } m_i &gt; 1.\n\\end{cases}\n\nWhen the project cost is high compared to the cost of capital, the principal should transfer the minimum acceptable utility. On the other hand, when the cost is in a profitable range, the principal should increase the transfer amount to fund the profitable investment, leading to largest cash flows."
  },
  {
    "objectID": "private_inf.html#communinication-strategies",
    "href": "private_inf.html#communinication-strategies",
    "title": "Private Information and Incentives",
    "section": "2.2 Communinication Strategies",
    "text": "2.2 Communinication Strategies\nSuppose the agent sees signal m_i but reports a higher cost m_j to obtain more the resources than needed ((m_j-m_i)x_j+\\underline{U}). However, there is a limit to how much he will overstate. If the agent claims the cost is too high (over 1), the principal will not provide any resources. Thus, the agent will report a cost slightly under 1 for all m_i&lt;1. If the agent observes a cost realization above 1, there is no incentive to lie because the project will not be funded anyway. As previously mentioned, the principal does not need to worry about the agent understating their costs. If he does, he will not receive enough resources to produce the required output, so any lie in this direction will always be uncovered.\n\n2.2.1 Principal’s Problem\nWe principal’s problem now includes creating a set of contract (x_,z_) that depends on the reported signal, z_i(\\hat{m}) such that the agent’s best reporting strategy is to tell the truth. The principal’s problem is then\n\n\\begin{aligned}\n    \\max_{x_i, z_i} \\quad & \\sum_{i=1}^{N} (x_i - z_i) g_i \\\\\n    \\text{subject to} \\quad & z_i - m_i x_i \\geq \\underline{U} \\quad \\text{for all } i = 1, \\dots, N, \\\\\n    & z_i - m_i x_i \\geq z_j - m_i x_j \\quad \\text{for all } i, j, \\\\\n    & 0 \\leq x_i \\leq x_{\\max}, \\\\\n    & 0 \\leq z_i.\n\\end{aligned}\n\nNotice that this version adds the second set of restrictions, which ensures that the agent does not have an incentive to lie about the cost of the project (truth-telling constraints, or self-section restriction), meaning that the agent observes the cost m_i he is better off with the contract (x_i,z_i) than with any other combitation (x_j,z_j)."
  },
  {
    "objectID": "private_inf.html#communication-strategies",
    "href": "private_inf.html#communication-strategies",
    "title": "Private Information and Incentives",
    "section": "2.3 Communication Strategies",
    "text": "2.3 Communication Strategies\nSuppose the agent privately sees signal m_i but reports a higher cost m_j to obtain more the resources than needed ((m_j-m_i)x_j). However, there is a limit to how much he will overstate. If the agent claims the cost is too high (over 1), the principal will not provide any resources. Thus, the agent will report a cost slightly under 1 for all m_i&lt;1. If the agent observes a cost realization above 1, there is no incentive to lie because the project will not be funded anyway. As previously mentioned, the principal does not need to worry about the agent understating the costs. If he does, the project will not receive the minimal funding to produce the required output, so any lie in this direction will always be uncovered.\n\n2.3.1 Principal’s Problem\nThe principal’s problem is to define a set of contract (x_i,z_i) that depends on the reported signal, z_i(\\hat{m}) such that the agent’s best reporting strategy is to tell the truth. The principal’s problem is then\n\n\\begin{aligned}\n    \\max_{x_i, z_i} \\quad & \\sum_{i=1}^{N} (x_i - z_i) g_i \\\\\n    \\text{subject to} \\quad & z_i - m_i x_i \\geq \\underline{U} \\quad \\text{for all } i = 1, \\dots, N, \\\\\n    & z_i - m_i x_i \\geq z_j - m_i x_j \\quad \\text{for all } i, j, \\\\\n    & 0 \\leq x_i \\leq x_{\\max} \\quad \\text{for all } i, \\\\\n    & 0 \\leq z_i.\n\\end{aligned}\n\\tag{2}\nNotice that this version adds the second set of N^2 restrictions, which ensures that the agent does not have an incentive to lie about the cost of the project (the ICCs, also called truth-telling constraints). Obviously, the agent’s reported costs m_j does not affect the underlying true cost of the project m_i. This is why the true cost m_i is at each side of the ICCs. In other words, the ICCs implies that when the agent observes the cost m_i, he is better off with the contract (x_i,z_i) than with any other combitation (x_j,z_j). This set of constraints can be reduced to the set of 2(N - 1) “adjacent” constraints, meaning that the manager must prefers the communicate the true cost to the cost immediately above and the cost immediately below dominates.\nThe upper bound restrictions in the cash flow can also be simplified to a single restriction x_N\\leq X_{\\max}.\nThe agent’s slack z_i-m_i x_i, decreases weakly in i because as m_i​ increases (i.e., higher costs), both x_i​ and z_i decreases in i. In the worst realization of i, the minimum acceptable utility is is z_N-m_N x_N=\\theta.\nLet k^* be the value of i that maximizes the principal’s problem. This implies that x_i=x_{\\max} for i=1,\\ldots,k^* and x_i=0 for all i=k^*+1,\\ldots,N. Since z_1=m_1 x_1+\\theta, and then z_{i}-z_{i-1}=m_i(x_i-x_{i-1}) for i=2, \\ldots, N, the transfer policy is\n\nz_i =\n\\begin{cases}\n    m_{k^*} x_{\\max} & \\text{for } i=1,...,k^*, \\\\\n    0 & \\text{for } i=k^*+1,...,N\n\\end{cases}\n\nCompare with the first best, we see that here the agent receives slack for all cost realization below the threshold (m_{k^*} x_{\\max}&gt;m_{i} x_{\\max} for all i&lt;k^*). The principal provides additional more funds than the actual investment cost to incentive the agent to reveal the truth. But, how much is the new cutoff? the cutoff is less than 1.0. This implies that the optimal solution involves the principal rejecting certain profitable projects (capital rationing). When the principal adjusts the cutoff, it impacts the flexibility the agent has for all lower levels below that point. Therefore, the principal’s trade-off is to balance the cost of setting the cutoff too high (paying for slack) and too low (foregoing profitable projects).\nWe can see this by solving numerically the linear program in Equation 2 considering the real cost m being a uniform discrete variable defined between 0.1 and 1.5, and \\theta=0.\n\n\nCode\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\nimport numpy as np\nfrom scipy.optimize import linprog\nimport pandas as pd\n\n# Parameters\nm = np.arange(0.1, 1.6, 0.1)\nN = len(m)\ntheta = 0\nx_max = 10\ng = np.array([1/N] * N) # Uniform distribution of signals\n\n# Objective function: maximize sum of (x_i - z_i) * g_i\n# Notice this is equivalent to minimiza  sum of z_i * g_i -x_i * g_i \n# c is a one-dimensional vector of size 2N: [x_1, z_1, x_2, z_2, ..., x_N, z_N]\nc = np.zeros(2 * N)\nfor i in range(N):\n    c[2 * i] = -g[i]  # Coefficient for x_i\n    c[2 * i + 1] = g[i]  # Coefficient for z_i\n\n# Inequality constraints for participation and incentive compatibility\nA = []\nb = []\n\n# Participation constraints: z_i - m_i * x_i &gt;= theta (agent's minimal utility constraint)\n# The constraint is a one-dimensional vector of size 2N to multiply with [x_1, z_1, x_2, z_2, ..., x_N, z_N]\nfor i in range(N):\n    constraint = [0] * (2 * N)  # Create a 2N-dimensional row, for matching dimension with ICCs\n    constraint[2 * i] = -m[i]   # Coefficient for x_i\n    constraint[2 * i + 1] = 1   # Coefficient for z_i\n    A.append(constraint)\n    b.append(theta)\n\n# Incentive compatibility constraints I: [z_i - m_i * x_i] -[z_{i-1} - m_{i} * x_{i-1}] &gt;= 0 \n# The constraint is a one-dimensional vector of size 2N to multiply with [x_1, z_1, x_2, z_2, ..., x_N, z_N]\nfor i in range(1, N):\n    constraint = [0] * (2 * N)  # Create a 2N-dimensional row\n    constraint[2 * i] = -m[i]  # Coefficient for x_i\n    constraint[2 * i + 1] = 1  # Coefficient for z_i\n    constraint[2 * (i - 1)] = m[i]  # Coefficient for x_{i-1}\n    constraint[2 * (i - 1) + 1] = -1  # Coefficient for z_{i-1}\n    A.append(constraint)\n    b.append(0)\n\n# Incentive compatibility constraints II: [z_i - m_i * x_i]-[z_{i+1} - m_{i} * x_{i+1}] &gt;= 0 \nfor i in range(N - 1):\n    constraint = [0] * (2 * N)  # Create a 2N-dimensional row\n    constraint[2 * i] = -m[i]  # Coefficient for x_i\n    constraint[2 * i + 1] = 1  # Coefficient for z_i\n    constraint[2 * (i + 1)] = m[i]  # Coefficient for x_{i+1}\n    constraint[2 * (i + 1) + 1] = -1  # Coefficient for z_{i+1}\n    A.append(constraint)\n    b.append(0)\n\n# Convert A and b to numpy arrays for linprog function: A*constraints &lt;= b\nA = np.array(A) * -1  # Multiply A by -1 to convert the inequality to the form Ax &lt;= b\nb = np.array(b) * -1  \n\n# %%\n# Bounds: 0 &lt;= x_i &lt;= x_max, 0 &lt;= z_i\nx_bounds = [(0, x_max)] * N\nz_bounds = [(0, None)] * N\nbounds = []\nfor i in range(N):\n    bounds.append(x_bounds[i])\n    bounds.append(z_bounds[i])\n\n# Solve the linear programming problem\nres = linprog(c, A_ub=A, b_ub=b, bounds=bounds, method='highs')\n\n# %%\n# Check if the optimization was successful\nif res.success:\n    x = res.x[::2]  # Extract the budget values from the solution, they are in even indices\n    z= res.x[1::2]\n    # the slack is transfers{i} -m{i}*cashflow{i}\n    slack =  z- m *   x\n    investment= m * x\n    # Create a dataframe for the results\n    df = pd.DataFrame({'Cost m': m,  'Budget Z': z, \"Investment\": investment , 'Slack': slack, 'Cash flow X': x,})\n    # Print the results as markdown using the tabulate package\n    display(Markdown(tabulate(df, headers='keys', tablefmt='pipe', showindex=False)))\nelse:\n    print(\"Optimization failed. Check the constraints or problem formulation.\")\n\n\n\n\nTable 1: Numerical Example\n\n\n\n\n\n\nCost m\nBudget Z\nInvestment\nSlack\nCash flow X\n\n\n\n\n0.1\n5\n1\n4\n10\n\n\n0.2\n5\n2\n3\n10\n\n\n0.3\n5\n3\n2\n10\n\n\n0.4\n5\n4\n1\n10\n\n\n0.5\n5\n5\n0\n10\n\n\n0.6\n-0\n-0\n0\n-0\n\n\n0.7\n-0\n-0\n0\n-0\n\n\n0.8\n-0\n-0\n0\n-0\n\n\n0.9\n-0\n-0\n0\n-0\n\n\n1\n0\n-0\n0\n-0\n\n\n1.1\n0\n-0\n0\n-0\n\n\n1.2\n0\n-0\n0\n-0\n\n\n1.3\n0\n-0\n0\n-0\n\n\n1.4\n0\n-0\n0\n-0\n\n\n1.5\n0\n-0\n0\n-0\n\n\n\n\n\n\n\n\nIn Table 1 we can see the two key insights from this type of models.\n\nThe optimal cutoff is m_{k^*}=0.5, which implies that the principal will not fund any project with a minimal cost above 0.5. Notice that for 0.6\\leq m &lt; 1, the profits of the investment outweight the costs, so, the principal is rationing the resources.\nFor m\\leq 0.4, the principal still transfers m_{k^*}x_{max} to the agent, which is the more the resources needed for the project. The agent is rewarded for communicating the the true minimal cost of the project."
  },
  {
    "objectID": "private_inf.html#principal-and-agents-preferences",
    "href": "private_inf.html#principal-and-agents-preferences",
    "title": "Private Information and Incentives",
    "section": "2.1 Principal and Agent’s Preferences",
    "text": "2.1 Principal and Agent’s Preferences\nThe principal is risk-neutral with utility function G(x,z)=x - z. The agent is also risk neutral with utility function U(w)=z-m_ix, meaning the money transfered by the principal minus the cost project observed privately. As in the initial setting, we assume that the agent can leave after observing a bad signal. Notice that the principal makes the decision on the transference z needed to obtain x based on the communication of the reported costs \\hat{m} provided at the beginning."
  },
  {
    "objectID": "private_inf.html#suggestions-for-further-reading",
    "href": "private_inf.html#suggestions-for-further-reading",
    "title": "Private Information and Incentives",
    "section": "2.4 Suggestions for Further Reading",
    "text": "2.4 Suggestions for Further Reading\nSeveral studies have look at budgeting capital investment in multiple time periods in contexts where the manager has better informaton, see for example, Rogerson 1997. A benefit of looking at multi-period settings, is that we can differentiate cash flows from accounting earnings. In that sense,Reichelstein (2000) compares performance measures based only on realized cash flows with the residual accounting income; while Durra and Reichelstein (2002) apply the inter-temporal setting to analyze the optimal capital charge for investment decision for divisional managers with superior information."
  }
]